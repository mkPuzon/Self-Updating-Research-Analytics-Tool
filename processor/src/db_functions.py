'''db_functions.py

Dumps metadata from .json file to PostgreSQL database with modular design and verbose logging.

Aug 2025'''
import os
import re
import glob
import json
import sqlite3
from datetime import datetime
from dotenv import load_dotenv

# ===== Utility Functions =====
def clean_text(text):
    """Remove null bytes and other problematic characters from text."""
    if not isinstance(text, str):
        return text
    try:
        # First, handle surrogate pairs by replacing them
        text = text.encode('utf-8', 'surrogatepass').decode('utf-8', 'replace')
        # Remove control characters except newlines and tabs
        text = re.sub(r'[\x00-\x08\x0B\x0C\x0E-\x1F\x7F\uFFFD]', '', text)
        # Replace any remaining problematic Unicode characters
        text = text.encode('ascii', 'ignore').decode('ascii', 'ignore')
        return text
    except Exception as e:
        # If any error occurs during cleaning, return an empty string
        print(f"Warning: Error cleaning text: {str(e)}")
        return ""

def get_db_connection(verbose=False):
    """Create and return a database connection."""
    try:
        conn = sqlite3.connect(DB_NAME)
        # use Write-Ahead-Logging (WAL)
        conn.execute("PRAGMA journal_mode=WAL;")
        conn.execute("PRAGMA busy_timeout=5000;")
        if verbose:
            print(f"Connection to {DB_NAME} successful.")
        return conn
        
    except Exception as e:
        print(f"[ERROR] Database connection failed: {e}")
        raise e

def setup_db(db_path):
    try:

        with sqlite3.connect(db_path) as conn:
            print(f"Opened SQLite db w/ version {sqlite3.sqlite_version}")

            cursor = conn.cursor()
            
            create_table_articles = '''
            CREATE TABLE IF NOT EXISTS articles (
                article_id INTEGER PRIMARY KEY,
                uuid TEXT,
                title TEXT,
                date_submitted TEXT,
                date_scraped TEXT,
                tags TEXT,
                authors TEXT,
                abstract TEXT,
                pdf_url TEXT,
                full_arxiv_url TEXT,
                full_text TEXT,
                keywords TEXT
            );
            '''
            create_table_keywords = '''
            CREATE TABLE IF NOT EXISTS keywords (
                keyword TEXT PRIMARY KEY,
                definition TEXT,
                count INTEGER DEFAULT 1,
                paper_references TEXT
            );
            '''
            cursor.execute(create_table_articles)
            cursor.execute(create_table_keywords)
            conn.commit()

    except sqlite3.OperationalError as e:
        print("[ERROR] Failed to open db:", e)

def clean_and_transform(key, raw_data):
    article_id = int(key)

    # convert strings to lists
    tags_raw = raw_data.get('tags', '')
    tags_list = [t.strip() for t in tags_raw.split(',')] if tags_raw else []
    authors_raw = raw_data.get('authors', '')
    authors_list = [a.strip() for a in authors_raw.split(',')] if authors_raw else []

    # convert unix timestamp to string
    try:
        ts = raw_data.get('date_scraped')
        if ts:
            ds = datetime.fromtimestamp(float(ts)).strftime('%Y-%m-%d')
        else:
            ds = None
    except ValueError:
        ds = None

    keyword_list = raw_data.get('keywords', [])

    return (
        article_id,
        raw_data.get('uuid'),
        raw_data.get('title'),
        raw_data.get('date_submitted'),
        ds,
        json.dumps(tags_list),    # Store as JSON string
        json.dumps(authors_list), # Store as JSON string
        raw_data.get('abstract'),
        raw_data.get('pdf_url'),
        raw_data.get('full_arxiv_url'),
        raw_data.get('full_text'),
        json.dumps(keyword_list)
    )

def process_file(data_dir):

    with get_db_connection() as conn:
        cursor = conn.cursor()
        total_inserted = 0

        print(f"Processing {os.path.basename(data_dir)}...")
        try:
            with open(data_dir, 'r', encoding="utf-8", errors="replace") as f:
                data = json.load(f)
            
            batch_data = []
            for key, entry in data.items():
                clean_row = clean_and_transform(key, entry)
                batch_data.append(clean_row)

            # bulk insert data
            cursor.executemany('''
                INSERT OR IGNORE INTO articles (
                    article_id, uuid, title, date_submitted, date_scraped, 
                    tags, authors, abstract, pdf_url, full_arxiv_url, full_text, keywords
                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                ''', batch_data)
            
            total_inserted += cursor.rowcount
            conn.commit()

        except Exception as e:
            print(f"Error processing {data_dir}: {e}")
    
    print(f"---- Total rows inserted: {total_inserted}")

def dump_metadata_to_db(json_filepath, db_path, verbose=False):
    '''Adds data to SQLite db from today's .json metadata file.'''
    
    setup_db(db_path=db_path)

    with sqlite3.connect(db_path) as conn:
        # Check for duplicates (Title OR UUID)
        sql_check_duplicate = "SELECT article_id FROM articles WHERE title = ? OR uuid = ?"

        sql_insert_articles = """
            INSERT INTO articles (
                uuid, title, date_submitted, date_scraped, tags, authors,
                abstract, pdf_url, full_arxiv_url, full_text, keywords
            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        """

        # Keyword Management
        sql_check_keyword = "SELECT count, paper_references FROM keywords WHERE keyword = ?"
        sql_update_keyword = "UPDATE keywords SET count = ?, paper_references = ? WHERE keyword = ?"
        sql_insert_keyword = "INSERT INTO keywords (keyword, definition, count, paper_references) VALUES (?, ?, 1, ?)"

        try:
            with open(json_filepath, "r") as f:
                data = json.load(f)

            cur = conn.cursor()

            for paper in data.values():
                # NOTE: If anything fails inside this loop, we roll back ONLY this paper
                conn.execute("BEGIN") 

                try:
                    definitions = paper.get('definitions', {})
                    if not isinstance(definitions, dict): definitions = {}
                    
                    definitions = {
                        str(k).strip(): str(v) if v is not None else ''
                        for k, v in definitions.items()
                        if v != "None" and v is not None and k is not None
                    }

                    if not definitions:
                        # if no definitions, we DO NOT add the paper to the db
                        conn.rollback()
                        continue

                    tags_list = [t.strip() for t in paper.get('tags', '').split(',') if t.strip()] if isinstance(paper.get('tags'), str) else []
                    authors_list = [a.strip() for a in paper.get('authors', '').split(',') if a.strip()] if isinstance(paper.get('authors'), str) else []
                    
                    keywords_raw = paper.get('keywords', [])
                    if isinstance(keywords_raw, str):
                        keywords_list = [k.strip() for k in keywords_raw.split(',') if k.strip()]
                    else:
                        keywords_list = [str(k).strip() for k in keywords_raw if k]

                    # Serialization helpers; SQLite cannot store array objects
                    clean_str = lambda x: x.strip() if isinstance(x, str) else x
                    json_list = lambda x: json.dumps([clean_str(i) for i in x])

                    # Check for duplicate papers
                    cur.execute(sql_check_duplicate, (paper.get('title', ''), paper.get('uuid', '')))
                    if cur.fetchone():
                        if verbose: print(f"Skipping duplicate: {paper.get('uuid', '')}")
                        conn.rollback() # Nothing happened, but good practice to reset
                        continue

                    # Insert articles
                    data_to_insert = (
                        clean_str(paper.get('uuid', '')),
                        clean_str(paper.get('title', '')),
                        clean_str(paper.get('date_submitted')),
                        paper.get('date_scraped'),
                        json_list(tags_list),
                        json_list(authors_list),
                        clean_str(paper.get('abstract')),
                        clean_str(paper.get('pdf_url')),
                        clean_str(paper.get('full_arxiv_url')),
                        clean_str(paper.get('full_text')),
                        json_list(keywords_list)
                    )
                    
                    cur.execute(sql_insert_articles, data_to_insert)
                    article_id = cur.lastrowid # Capture ID immediately

                    # Add keywords
                    for keyword, definition in definitions.items():
                        clean_kw = keyword.strip()
                        clean_def = definition.strip()
                        if not clean_kw: continue

                        cur.execute(sql_check_keyword, (clean_kw,))
                        row = cur.fetchone()

                        if row: # keyword exists in db already
                            current_count = row[0]
                            try:
                                current_refs = json.loads(row[1])
                            except:
                                current_refs = []
                            new_count = current_count

                            if str(article_id) not in current_refs:
                                current_refs.append(str(article_id))
                                new_count += 1 

                            cur.execute(sql_update_keyword, (
                                new_count, 
                                json.dumps(current_refs), 
                                clean_kw
                            ))
                        else: # new keyword
                            initial_refs = json.dumps([str(article_id)])
                            cur.execute(sql_insert_keyword, (clean_kw, clean_def, initial_refs))

                    conn.commit()
                    if verbose: print(f"Committed article {article_id}")

                except Exception as inner_e:
                    conn.rollback()
                    print(f"Error processing paper {paper.get('uuid', 'Unknown')}: {inner_e}")
                    continue

        except Exception as e:
            print(f"[CRITICAL ERROR] Failed to load JSON or DB connection: {e}")
        
if __name__ == "__main__":
    today = datetime.today().strftime('%Y-%M-%d')
    today = "2026-01-28"

    DATA_DIR = f'data/metadata/metadata_{today}.json'
    DB_NAME = 'data/aura.db'

    # setup_db(DATA_DIR)
    dump_metadata_to_db(DATA_DIR, DB_NAME, verbose=True)